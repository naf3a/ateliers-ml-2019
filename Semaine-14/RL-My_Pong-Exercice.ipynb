{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 14 - Reinforcement Learning, Policy Gradient\n",
    "\n",
    "Dans l'exercice de cette semaine, nous allons implémenter un joueur de Pong avec gym.\n",
    "\n",
    "Nous aurons besoin d'implémenter plusieurs fonctions afin de faire fonctionner la loop de jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction sigmoïd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertion d'un tableau à 3 dimensions représentant l'image du jeu, en un vecteur de 6400 (80x80) valeurs 1 ou 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui recalcule les rewards avec discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseau de neurone pour obtenir la probabilité de prendre une action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward(model, x):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return p, h # return probability of taking action 2, and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation, lors du training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_backward(model, epx, eph, epdlogp):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour créer une image négative représentant la différence entre l'image précédente et la nouvelle (conserve le mouvement entre les deux étapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the observation, set input to network to be difference image\n",
    "def difference_image(observation, prev_x):\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    return x, cur_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour le choix de l'action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward the policy network and sample an action from the returned probability\n",
    "def get_action(model, x):\n",
    "    aprob, h = policy_forward(model, x)\n",
    "    action = # ...\n",
    "    return action, aprob, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrit et récupère l'historique des inputs, hidden states, actions et rewards de l'épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record various intermediates (needed later for backprop)\n",
    "def record_history(xs, hs, dlogps, x, action, aprob, h):\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state for fitting\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "def get_history(xs, hs, dlogps, drs):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return epx, eph, epdlogp, epr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction appliquant le discount au reward et le normalisant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the discounted reward backwards through time\n",
    "def standardize_reward(epr, gamma):\n",
    "    discounted_epr = discount_rewards(epr, gamma)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "   \n",
    "    # ...\n",
    "\n",
    "    return discounted_epr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fonction du gradient ascent, utilisant l'optimisation rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizer function, you don't have to understand it, works like Adam\"\"\"\n",
    "def rmsprop_update(model, grad_buffer, rmsprop_cache, decay_rate):\n",
    "    for k,v in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "        return model, grad_buffer, rmsprop_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour mémoriser diverses données. Le modèle est conservé dans un fichier pour ne pas recommencer à 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boring book-keeping\n",
    "def book_keeping(model, episode_number, running_reward, reward_sum):\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('resetting env. episode reward total was %f. running mean: %f', reward_sum, running_reward)\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    return running_reward, reward_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du Pong\n",
    "\n",
    "Il n'y a plus qu'à tester !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = True\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "while True:\n",
    "    if render: env.render()\n",
    "\n",
    "    x, prev_x = difference_image(observation, prev_x)\n",
    "\n",
    "    action, aprob, h = get_action(model, x)\n",
    "    \n",
    "    record_history(xs, hs, dlogps, x, action, aprob, h)\n",
    "    \n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        epx, eph, epdlogp, epr = get_history(xs, hs, dlogps, drs)\n",
    "\n",
    "        discounted_epr = standardize_reward(epr, gamma)\n",
    "    \n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(model, epx, eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "        # perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            model, grad_buffer, rmsprop_cache = rmsprop_update(model, grad_buffer, rmsprop_cache, decay_rate)\n",
    "\n",
    "        running_reward, reward_sum = book_keeping(model, episode_number, running_reward, reward_sum)\n",
    "\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n",
    "        if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "            print('ep %d: game finished, reward: %f', episode_number, reward, '' if reward == -1 else ' !!!!!!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
